# Content for the blog

## Problem that we're trying to solve

The problem was to automatically estimate the cost of infrastructure changes from IaC code changes in pull requests. Most IaC platforms support getting a preview of changes before applying them. So the initial idea was to send the preview to a LLM and get a cost estimate.

## Solution

We developed a Github action that runs on every pull request. It looks for changes in the IaC code. If detected, it will run the platform specific command to get the preview of the changes. Then it will use a LLM to generate a cost estimate. This is fairly straightforward but comes with some challenges. Most of the infrastructure cost depends on the usage. But IaC state does not contain usage data. So we need to make some assumptions.

We estimated a base cost and three levels of usage costs (low, medium, high). Usage assumptions like incoming traffic, uptime, etc are generated by the LLM.

## Cost per run

Preview jsons from IaC platforms tend to be fairly large. A sample preview json we used to calculate the cost was 6100 tokens. A **video processing pipeline** deployed on AWS using Terraform. The architecture includes **fixed-cost components** (EC2, RDS, EIP) and **variable-cost components** (S3, Lambda, CloudWatch). Here are the LLM costs per run:

-   GPT-4o
    -   Input tokens (6,100): $0.015 (at $2.5/1M tokens)
    -   Output tokens (1,020): $0.012 (at $10/1M tokens)
    -   Total: $0.027 per run
-   GPT-4-mini
    -   Input tokens (6,100): $0.000915 (at $0.15/1M tokens)
    -   Output tokens (1,020): $0.00072 (at $0.6/1M tokens)
    -   Total: $0.001635 per run

Costs can be further reduced by using cheaper models. But cheaper the model, the more inaccurate the cost estimates are. We did an analysis of the accuracy for different models.

-   Gemma 2 9B
    -   Inconsistent JSON formatting
-   LLaMA 3.2 1B Preview
    -   Simply repeats input
-   LLaMA 3.2 3B Preview
    -   Produces properly formatted JSON
-   GPT-4o
    -   Properly formatted JSON output
    -   Fairly accurate cost estimates
    -   Slight variations between runs due to usage assumptions
    -   Higher cost per run
-   GPT-4o-mini
    -   Similar performance to GPT-4o
    -   Properly formatted JSON output
    -   Accurate cost estimates
    -   Lower cost per run
    -   Best balance of performance and cost

## Challenges in LLMs

Infrastructure changes are a closed set. Therefore deterministic and accurate cost calculations are actually possible. But that will take a lot of effort. Our experiment was to see how well LLMs can handle this. We encountered some problems and figured out some solutions.

1. Output Format Consistency
   The designed workflow requires the LLM to output a JSON object. But the LLM will sometimes output a description and then the JSON object. This is a problem because the application code needs to parse the JSON object. We tried different prompts but the method that worked best was to use a system prompt as follows.

> You are a helpful assistant that only speaks JSON.

2. Calculation Accuracy

LLMs are not good at precise calculations. They are good at reasoning and assumptions. And a sub-optimal approach is to ask the LLM to generate individual costs and their aggregations. There is an information redundency here and since LLMs are stochastic, the results are not consistent. So we improved the prompt and response format to make sure that the LLM will not have to generate the same information twice. The derived information are later calculated by the application code.

3. Cost assumption accuracy

The reasoning models approach in such a way that it discusses the approach and then finding the answer. That chain of thought makes them more accurate. But it also makes them expensive. Following the same idea we changed the response format our non reaasoning models. In the json, before each cost estimate, there's a string field that describes the cost. So instead of the model one shot generating the cost, it will have some background information to make the cost calculation. Even then only the State-of-the-art models are able to generate fairly accurate cost estimates.

## Better solutions

Hardest part of building a deterministic cost estimator is to get the pricing data. One way is to use a browser agent to visit relevent websites and scrape the pricing data. But this is time consuming and expensive. Manually doing this will take too much effort and defeats the goal of developing a quick solution. Ideally, cost calculations should be deterministic and LLMs are only used for usage assumptions.

Another approach to reduce the LLM cost is to get a fairly usable smaller model and finetune it for this specific use case. We have not explored this approach yet.
